---
title: "dsc282w_asmt1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Exploratory

#### load data
```{r data}
load("data/samsungData.rda") # load data
dim(samsungData)
```

#### dimension
```{r dim-data}
dimdata = dim(samsungData)
dim(samsungData)
```

#### list all features 
- note that column number 562 is subject (number), and 563 is class label
```{r ls-features}
colnames(samsungData)
```

#### factorize the class labels
```{r factor}
samsungData[,dimdata[2]] = factor(samsungData[,dim(samsungData)[2]])
# check if is factor
is.factor(samsungData[,dimdata[2]])
```

#### Fix duplicated column names
```{r fix-duplicates}
duplicated_index = which(duplicated(colnames(samsungData)))
duplicated_columns =unique(colnames(samsungData)[(duplicated(colnames(samsungData)))])

samsungData.new = samsungData[,!(duplicated(colnames(samsungData)))]
dim(samsungData.new)

## adding .index to each duplicated column
for (each in duplicated_columns){
  ind = which(colnames(samsungData)==each)
  colnames(samsungData)[ind]= unlist(lapply(1:length(ind), function(i) paste(colnames(samsungData)[ind[i]], i, sep=".") ))
}
# view duplicated columns names
colnames(samsungData)[duplicated_index]
write(duplicated_index, "duplicated_column_indices.txt", sep="\n")

```

### summary of class label 
```{r check-class-label}
table(samsungData[, dimdata[2]])
```

### summary of subject numbers
```{r check-subject-numbers}
table(samsungData[, dimdata[2]-1])
barplot(summary(factor(samsungData[, dimdata[2]-1])), cex.names =0.6)
title("number of data points for each subject")
```

### summary of class labels per subject
```{r check-class-per-subject}
my_df = table(samsungData$subject, samsungData$activity)
par(cex.main = 1)
plot(my_df, main="")
title(main="Distribution of Number of Records of Each Activity For Each Subject", outer = FALSE)
barplot(t(as.matrix(my_df)), beside = TRUE, main = "number of datapoints for each activity per subject", col = c(3,4,6,7,3,1))
legend("topright", colnames(my_df), col = c(3,4,6,7,3,1), lty = 1, lwd = 2, cex = 0.5)
```

##### check balance
```{r check balance}
boxplot.matrix(t(my_df), xlab="subject", ylab="count", main="Number of instances per activity for each subject")
lb  = c(names(which.max(my_df['1',])),names(which.max(my_df['30',])))
text(c(1,21), c(max(my_df['1',]), max(my_df['30',])), lb, col="red", pos=c(4,3), cex=0.9)
```

- ggplot version
```{r}

## ggplot2
library(reshape2)
library(ggplot2)
melted = melt(my_df)
colnames(melted)[1:2] = c("subject","activity")
lab = c(names(which.max(my_df["1",])),names(which.max(my_df["15",])),names(which.max(my_df["30",])))
where = c(max(my_df["1",]),max(my_df["15",]), max(my_df["30",]))
ggplot(melted, aes(factor(subject), value))+geom_boxplot()+labs(title = "Number of Instances Per Activity For Each Subject", x="subject", y="count")+geom_text(aes("1",where[1]+2), label=lab[1], color="blue")+geom_text(aes("30",where[3]+2), label=lab[3], color="blue")+geom_text(aes("15",where[2]+2), label=lab[2], color="blue")

```

## distribution of activities per subject
```{r}

my_df.2 = t(apply(my_df, 1, function(x) x/sum(x)))
melted.2 = melt(my_df.2)
colnames(melted.2)[1:2] = c("subject","activity")
lab = c(names(which.max(my_df["1",])),names(which.max(my_df["15",])),names(which.max(my_df["30",])))
where = c(max(my_df["1",]),max(my_df["15",]), max(my_df["30",]))
ggplot(melted.2, aes(factor(subject), value))+geom_boxplot()+labs(x="subject", y="ratio")+scale_y_continuous()
melted.2$subject = factor(melted.2$subject)
summary(aov(value ~ subject, data = melted.2))

```


### ANOVA
- p val > 0.5. Therefore, cannot reject H0 that means are equal at alpha = 0.05
- therefore, we decided to split train and test groups by subjects
```{r aov num of instances per activity per subject}
my_df_aov = aov(value~factor(subject),data = melted)
summary(my_df_aov)

my_df_aov.2 = aov(value~factor(activity),data = melted)
summary(my_df_aov.2)
```

### boxplot
- see the range of data for each feature
```{r boxplot: range of data for each feature, echo=TRUE}
last_feature_index = dimdata[2]-2
for (i in seq(1,last_feature_index, 50)) {
  if (i+50 < last_feature_index) {
    end = i + 50
  } else {
    end =last_feature_index
  }
  boxplot(samsungData[,i:end], main=paste("[",i, ",",(i+50), "]"))
}
```
#### boxplot
```{r boxplot my_df}
boxplot.matrix(my_df)
```

### variance across samples per feature
```{r variance}
par(mfrow=c(1,1))
var_per_feature = apply(samsungData[,1:561], 2, var)

barplot(var_per_feature, axisnames = F, cex.names = 0.8, cex.axis = 0.8, xlab = "feature", main = "Variances")


mean_per_feature = apply(samsungData[,1:561], 2, mean)

barplot(mean_per_feature, axisnames = F, cex.names = 0.8, cex.axis = 0.8, xlab = "feature", main = "Means")


cv_per_feature = apply(samsungData[,1:561], 2, function(x) sd(x)/mean(x))

barplot(cv_per_feature, axisnames = F, cex.names = 0.8, cex.axis = 0.8, xlab = "feature", main = "CVs")

abs_cv_per_feature = apply(samsungData[,1:561], 2, function(x) abs(sd(x)/mean(x)))

barplot(abs_cv_per_feature, axisnames = F, cex.names = 0.8, cex.axis = 0.8, xlab = "feature", main = "Absolute CVs")

sorted_variances = var_per_feature[order(var_per_feature, decreasing = T)] 

sorted_abs_cvs = abs_cv_per_feature[order(abs_cv_per_feature, decreasing = T)] 

# pick top 5
top_5_sorted_variances = sorted_variances[1:5]
top_5_sorted_abs_cvs = sorted_abs_cvs[1:5]
names_top_5_sorted_variances = names(top_5_sorted_variances)
names_top_5_sorted_abs_cvs = names(top_5_sorted_abs_cvs )
```

### view the top 5
```{r view}
top_5_sorted_variances
top_5_sorted_abs_cvs
```


### Split data into training and test set
- split by subjects
- ramdomly select 80% for train and 20% for test

```{r split-function, echo=TRUE}

split_data = function (data, seed) {
  set.seed(seed) # set seeed for reproducibility
  subject_list = attributes(factor(data[,dim(data)[2]-1]))$levels
  training_ratio = 0.8
  n_train = floor(training_ratio*length(subject_list))
  trainning_subjects = sample(subject_list, n_train, replace = FALSE)
  trainning_indices <<- which( data[,"subject"]%in%trainning_subjects)
  test_subjects = subject_list[!(subject_list%in%trainning_indices)]
  
  
  test_indices <<- which(!data[,"subject"]%in%trainning_subjects)
  
  write.table(data, file = "samsungData_fixed-duplicated-columns.csv", row.names = FALSE, col.names = TRUE, sep = "," )
  
  write.table(data[trainning_indices,], file = "samsungData_fixed-duplicated-columns.train.csv", row.names = FALSE, col.names = TRUE, sep = "," )
  
  write.table(data[test_indices,], file = "samsungData_fixed-duplicated-columns.test.csv", row.names = FALSE, col.names = TRUE, sep = "," )
  
}
```

```{r split}
split_data(data = samsungData, seed = 123)
```

## Feature Selection

### Train
- using Random forest in h2o.ai package
- Does random forest perform worse with the normalized data?
- 10-fold cross-validation
- use seed = 123 for reproducibility reason

```{r start h2o, results='hide', eval=FALSE}
# The following two commands remove any previously installed H2O packages for R.
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }

# Next, we download packages that H2O depends on.
if (! ("methods" %in% rownames(installed.packages()))) { install.packages("methods") }
if (! ("statmod" %in% rownames(installed.packages()))) { install.packages("statmod") }
if (! ("stats" %in% rownames(installed.packages()))) { install.packages("stats") }
if (! ("graphics" %in% rownames(installed.packages()))) { install.packages("graphics") }
if (! ("RCurl" %in% rownames(installed.packages()))) { install.packages("RCurl") }
if (! ("jsonlite" %in% rownames(installed.packages()))) { install.packages("jsonlite") }
if (! ("tools" %in% rownames(installed.packages()))) { install.packages("tools") }
if (! ("utils" %in% rownames(installed.packages()))) { install.packages("utils") }

# h2o_3.8.2.6
install.packages("h2o", type="source", repos=(c("https://h2o-release.s3.amazonaws.com/h2o/rel-turchin/6/R")))
```

#### load h2o module and start h2o node in the local machine 
```{r load h2o module}
## load modules and start h2o compute node
library(h2o)
localH2O = h2o.init(ip = "localhost",  startH2O = TRUE)
```

```{r feature-selection-random-forest,  results='hide'}
# upload file to h2o 
samsungData.hex = h2o.uploadFile(path = "samsungData_fixed-duplicated-columns.train.csv")
dim(samsungData.hex)
ncol = dim(samsungData.hex)[2]
x = colnames(samsungData.hex)[-((ncol-1):ncol)]
y = colnames(samsungData.hex)[ncol]

# classification with random forest, and get the top most important features used
list_models = read.table("model_id.txt", sep="\t", header = FALSE,col.names = c('model', 'model_id'), stringsAsFactors = FALSE)
rownames(list_models) = list_models[,1]
list_models = list_models[-1]

```

```{r try full}
tryCatch ( {
  model <<- h2o.getModel(list_models['full model',])},  # <<- save to global 
  error=function(e) {
    model <<- h2o.randomForest(x, y, seed = 123, samsungData.hex, nfolds = 10)
    write(paste("full model",model@model_id, sep="\t"), "model_id.txt", append = FALSE)} )

```

### try no cross-validation
```{r no cv}
model_no_cross_validate = h2o.randomForest(x, y, seed = 123, samsungData.hex)
```
- cross-validated model
```{r cross-validated model}
vars_from_cv = h2o.varimp(model)[1:5,'variable']
```
- no-cross-validated model
```{r no-cross-validated model}
vars_from_no_cv = h2o.varimp(model_no_cross_validate)[1:5,'variable']
identical(vars_from_cv, vars_from_no_cv) # true
```
##### compare runtime
```{r runtime cv vs no-cv}
model_no_cross_validate @model$run_time # 18 sec
model@model$run_time # 25 sec
```


#### baseline accuracy
##### confusion matrix
```{r baseline-accuracy}
h2o.confusionMatrix(model)
```

##### total accuracy
```{r baseline-confusion-matrix}
1 - h2o.confusionMatrix(model)['Totals','Error']
```

### accuracy across cv
```{r accuracy-model-cv}
baseline_accuracy_cv = model@model$cross_validation_metrics_summary[,c('mean', 'sd')]
baseline_accuracy = baseline_accuracy_cv['accuracy',]
```

### Hit ratio
- what is it?

###### check varimp across cv
- The result confirmed that the algorithm selected the variables that are most common across 10-fold cross-validation
```{r 10 fold varimp-model}
compared_top_imp_vars =matrix(nrow=5, ncol=0)
for (m in h2o.cross_validation_models(model)){
  compared_top_imp_vars = cbind(compared_top_imp_vars, h2o.varimp(m)$variable[1:5])
}
compared_top_imp_vars = cbind(compared_top_imp_vars,h2o.varimp(model)$variable[1:5])
colnames(compared_top_imp_vars) = c(1:10, "selected")
rownames(compared_top_imp_vars) = paste("var",c(1:5), sep="")
t(as.data.frame(compared_top_imp_vars))
```

##### select top features for to train classification models (random forest)
```{r feature-selection-top,  results='hide'}
# select top features
top5_important_feature = model@model$variable_importances$variable[1:5]
top4_important_feature = model@model$variable_importances$variable[1:4]
top3_important_feature = model@model$variable_importances$variable[1:3]
top2_important_feature = model@model$variable_importances$variable[1:2]
top1_important_feature = model@model$variable_importances$variable[1]

### Train models, starting from using top 3 importand features until 80% accuracy is acchieved

tryCatch( {
  model.with.1.features<<- h2o.getModel(list_models['1-featured model',])
  model.with.2.features<<- h2o.getModel(list_models['2-featured model',])
  model.with.3.features<<- h2o.getModel(list_models['3-featured model',])
  model.with.4.features<<- h2o.getModel(list_models['4-featured model',])
  model.with.5.features<<- h2o.getModel(list_models['5-featured model',])
}, error = function(e) {
  model.with.1.features <<- h2o.randomForest(top1_important_feature, y, seed = 123, samsungData.hex, nfolds = 10)
  model.with.2.features <<- h2o.randomForest(top2_important_feature, y, seed = 123, samsungData.hex, nfolds = 10)
  model.with.3.features <<- h2o.randomForest(top3_important_feature, y, seed = 123, samsungData.hex, nfolds = 10)
  model.with.4.features <<- h2o.randomForest(top4_important_feature, y, seed = 123, samsungData.hex, nfolds = 10)
  model.with.5.features <<- h2o.randomForest(top5_important_feature, y, seed = 123, samsungData.hex, nfolds = 10)
  write(paste("1-featured model",model.with.1.features@model_id, sep="\t"), "model_id.txt", append = TRUE)
  write(paste("2-featured model",model.with.2.features@model_id, sep="\t"), "model_id.txt", append = TRUE)
  write(paste("3-featured model",model.with.3.features@model_id, sep="\t"), "model_id.txt", append = TRUE)
  write(paste("4-featured model",model.with.4.features@model_id, sep="\t"), "model_id.txt", append = TRUE)
  write(paste("5-featured model",model.with.5.features@model_id, sep="\t"), "model_id.txt", append = TRUE)
  
})


```

```{r compare varimp-model}
compared_top_imp_vars =matrix(nrow=5, ncol=0)
for (m in h2o.cross_validation_models(model)){
  compared_top_imp_vars = cbind(compared_top_imp_vars, h2o.varimp(m)$variable[1:5])
}
compared_top_imp_vars = cbind(compared_top_imp_vars,h2o.varimp(model)$variable[1:5])
colnames(compared_top_imp_vars) = c(1:10, "selected")
rownames(compared_top_imp_vars) = paste("var",c(1:5), sep="")
t(as.data.frame(compared_top_imp_vars))
```

##### train with variable selected from filter method
```{r filter-feature-selection-top,  results='hide'}
# select top features
top5_important_feature_filter_variance = names_top_5_sorted_variances
top5_important_feature_filter_abs_cv = names_top_5_sorted_abs_cvs
top4_important_feature_filter_variance = names_top_5_sorted_variances[1:4]
top4_important_feature_filter_abs_cv = names_top_5_sorted_abs_cvs[1:4]
top3_important_feature_filter_variance = names_top_5_sorted_variances[1:3]
top3_important_feature_filter_abs_cv = names_top_5_sorted_abs_cvs[1:3]

y = colnames(samsungData.hex)[ncol]
tryCatch( {
  # model.with.1.features<<- h2o.getModel(list_models['1-featured model',])
  # model.with.2.features<<- h2o.getModel(list_models['2-featured model',])
  # model.with.3.features<<- h2o.getModel(list_models['3-featured model',])
  # model.with.4.features<<- h2o.getModel(list_models['4-featured model',])
  model.with.5.features_filter_variance <<- h2o.getModel(list_models['5-featured model_filter_variance',])
    model.with.5.features_filter_abs_cv <<- h2o.getModel(list_models['5-featured model_filter_cv',])
      model.with.4.features_filter_variance <<- h2o.getModel(list_models['4-featured model_filter_variance',])
    model.with.4.features_filter_abs_cv <<- h2o.getModel(list_models['4-featured model_filter_cv',])
      model.with.3.features_filter_variance <<- h2o.getModel(list_models['3-featured model_filter_variance',])
    model.with.3.features_filter_abs_cv <<- h2o.getModel(list_models['3-featured model_filter_cv',])
    
}, error = function(e) {
  model.with.5.features_filter_variance <<- h2o.randomForest(top5_important_feature_filter_variance, y, seed = 123, samsungData.hex, nfolds = 10)
  model.with.5.features_filter_abs_cv <<- h2o.randomForest(top5_important_feature_filter_abs_cv, y, seed = 123, samsungData.hex, nfolds = 10)
    model.with.4.features_filter_variance <<- h2o.randomForest(top4_important_feature_filter_variance, y, seed = 123, samsungData.hex, nfolds = 10)
  model.with.4.features_filter_abs_cv <<- h2o.randomForest(top4_important_feature_filter_abs_cv, y, seed = 123, samsungData.hex, nfolds = 10)
    model.with.3.features_filter_variance <<- h2o.randomForest(top3_important_feature_filter_variance, y, seed = 123, samsungData.hex, nfolds = 10)
  model.with.3.features_filter_abs_cv <<- h2o.randomForest(top3_important_feature_filter_abs_cv, y, seed = 123, samsungData.hex, nfolds = 10)
  write(paste("5-featured model_filter_variance",model.with.5.features_filter_variance@model_id, sep="\t"), "model_id.txt", append = TRUE)
    write(paste("5-featured model_filter_cv",model.with.5.features_filter_abs_cv@model_id, sep="\t"), "model_id.txt", append = TRUE)
      write(paste("4-featured model_filter_variance",model.with.4.features_filter_variance@model_id, sep="\t"), "model_id.txt", append = TRUE)
    write(paste("4-featured model_filter_cv",model.with.4.features_filter_abs_cv@model_id, sep="\t"), "model_id.txt", append = TRUE)
      write(paste("3-featured model_filter_variance",model.with.3.features_filter_variance@model_id, sep="\t"), "model_id.txt", append = TRUE)
    write(paste("3-featured model_filter_cv",model.with.3.features_filter_abs_cv@model_id, sep="\t"), "model_id.txt", append = TRUE)
  
})


```

#### Accuracy
## RandomForest selected models
```{r compute-train-accuracy}
accuracy.randForest = data.frame(0,0)
colnames(accuracy.randForest) = c('mean','sd')
model_list = list( model.with.1.features,model.with.2.features,model.with.3.features,model.with.4.features,model.with.5.features)
for (i in 1:length(model_list)){
  accuracy.randForest[i,] = as.numeric(model_list[[i]]@model$cross_validation_metrics_summary['accuracy',c('mean','sd')])
}
accuracy.randForest['all',] = as.numeric( baseline_accuracy)
accuracy.randForest
```

##### Accuracy plot
```{r accuracy-plot}
# ggplot2
x  = rownames(accuracy.randForest)
y = accuracy.randForest[,1]
sd = accuracy.randForest[,2]
h = 0.8
qplot(x,y)+geom_errorbar(aes(x=x, ymin=y-sd, ymax=y+sd), width=0.25)+labs(title = "Accuracies of Models with Different Numbers of Features Used", x="number of features used", y="accuracy")+geom_hline(yintercept = h,lty = "dashed",show.legend = TRUE )+geom_text(aes("all",h,label = "threshold = 0.8", vjust = -1))

```

## filter selected models
##### selected by variances
```{r filter-compute-train-accuracy}
accuracy = data.frame(0,0)
colnames(accuracy) = c('mean','sd')
model_list_var = list( model.with.3.features_filter_variance,model.with.4.features_filter_variance,model.with.5.features_filter_variance)
for (i in 1:length(model_list_var)){
  accuracy[i,] = as.numeric(model_list_var[[i]]@model$cross_validation_metrics_summary['accuracy',c('mean','sd')])
}
accuracy['all',] = as.numeric( baseline_accuracy)
rownames(accuracy)[1:3] = 3:5 
accuracy
```

- Accuracy plot
```{r filter-accuracy-plot}
# ggplot2
x  = rownames(accuracy)
y = accuracy[,1]
sd = accuracy[,2]
h = 0.8
qplot(x,y)+geom_errorbar(aes(x=x, ymin=y-sd, ymax=y+sd), width=0.25)+labs(title = "Accuracies of Models with Different Numbers of Most-variant Features Used", x="number of features used", y="accuracy")+geom_hline(yintercept = h,lty = "dashed",show.legend = TRUE )+geom_text(aes("all",h,label = "threshold = 0.8", vjust = -1))

```

##### selected by absolute coeffients of variation (abs cv)
```{r filter2-compute-train-accuracy}
accuracy = data.frame(0,0)
colnames(accuracy) = c('mean','sd')
model_list_abs_cv = list( model.with.3.features_filter_abs_cv,model.with.4.features_filter_abs_cv,model.with.5.features_filter_abs_cv)
for (i in 1:length(model_list_abs_cv)){
  accuracy[i,] = as.numeric(model_list_abs_cv[[i]]@model$cross_validation_metrics_summary['accuracy',c('mean','sd')])
}
accuracy['all',] = as.numeric( baseline_accuracy)
rownames(accuracy)[1:3] = 3:5 
accuracy
```

- Accuracy plot
```{r filter2-accuracy-plot}
# ggplot2
x  = rownames(accuracy)
y = accuracy[,1]
sd = accuracy[,2]
h = 0.8
qplot(x,y)+geom_errorbar(aes(x=x, ymin=y-sd, ymax=y+sd), width=0.25)+labs(title = "Accuracies of Models with Different Numbers of Highest-Abs-CV Features Used", x="number of features used", y="accuracy")+geom_hline(yintercept = h,lty = "dashed",show.legend = TRUE )+geom_text(aes("all",h,label = "threshold = 0.8", vjust = -1))

```


#### view the selected features
- 5 features
- classification accuracy >= 0.8
```{r view-selected-features}
acc = accuracy.randForest[,"mean" ]
for (i in 1:length(acc)){
  this.accuracy = acc[i]
  if (this.accuracy >= 0.8) {
    num_features_selected <<- i
    break
  }
}
num_features_selected
selected_model = model_list[[num_features_selected]]
selected_features = top5_important_feature[1:num_features_selected]
```

#### view the confusion matrix of model using `r num_features_selected` features
```{r view-train-confusion-matrix}
h2o.confusionMatrix(selected_model)
```

#### accuracy across cv of the selected model
```{r accuracy-selected-model-cv}
selected_model@model$cross_validation_metrics_summary[,c('mean', 'sd')]
```

#### view details
```{r details}
selected_model@model$model_summary
```
##### view all stdout details
```{r out}
selected_model
```

## GLM 

# train with the 3 features selected from Random Forest
```{r glm}
# y = colnames(samsungData.hex)[ncol]
# is.factor(samsungData.hex[ncol])
# #install.packages("ade4")
# library(ade4)
# y.array = acm.disjonctif(samsungData[ncol])
# colnames(y.array) = names(summary(factor(samsungData[,ncol])))
# samsungData.glm = samsungData
# samsungData.glm = samsungData.glm[,-563] 
# samsungData.glm = cbind(samsungData.glm , y.array)
# write.table(samsungData.glm, file = "samsungData.glm.csv", row.names = FALSE, col.names = TRUE, sep = "," )
# 
#   write.table(samsungData.glm[trainning_indices,], file = "samsungData.glm.train.csv", row.names = FALSE, col.names = TRUE, sep = "," )  
# 
#   write.table(samsungData.glm[test_indices,], file = "samsungData.glm.test.csv", row.names = FALSE, col.names = TRUE, sep = "," )
# 
# samsungData.hex.glm  = h2o.uploadFile("samsungData.glm.train.csv")
# 
# ### how to do glm -- > multi label !!
# glm.model.train = h2o.glm(x=top3_important_feature, y = colnames(y.array), training_frame = samsungData.hex.glm, family = "binomial")
# 
# library(nnet)
# top3_important_feature
# ind = match(top3_important_feature, colnames(samsungData))
# indexed_samsumData = samsungData
# colnames(indexed_samsumData) = make.names(colnames(samsungData), unique = TRUE)
# top3_important_feature_glm = colnames(indexed_samsumData) [ind]
# train.glm = multinom(activity ~ tGravityAcc.mean...Y + tGravityAcc.min...X +angle.X.gravityMean.,data=indexed_samsumData[trainning_indices,])
# 
# z <- summary(train.glm)$coefficients/summary(train.glm)$standard.errors
# z
# #2-tailed z test
# p <- (1 - pnorm(abs(z), 0, 1))*2
# p # how does this work?
# exp(coef(train.glm))
# head(pp <- fitted(train.glm))
# 
# d.samsungData.train = indexed_samsumData[trainning_indices,c(42,53,559,563)]
# 
# train.predicted.glm = predict(train.glm, newdata = d.samsungData.train )
# 
# ## confusion matrix
# table(train.predicted.glm, indexed_samsumData[trainning_indices,563])
# 
# # accuracy
# sum(train.predicted.glm==indexed_samsumData[trainning_indices,563])/length(train.predicted.glm)
# 
# 
# ############## don't touch this
# 
# d.samsungData.test = indexed_samsumData[test_indices,c(42,53,559,563)]
# 
# test.predicted.glm = predict(train.glm, newdata = d.samsungData.test )


```


### Test
- using `r num_features_selected` selected features
```{r load-test-data-to-h2o}
samsungData.test.hex = h2o.uploadFile(path = "samsungData_fixed-duplicated-columns.test.csv")
dim(samsungData.test.hex)
# ncol = dim(samsungData.test.hex)[2]
# x =selected_features
# y = colnames(samsungData.test.hex)[ncol]
# 
# # classification of the test data with random forest
# tryCatch( {
#   model.test<<- h2o.getModel(list_models['test_model',])
# }, error = function(e) {
#   model.test <<- h2o.randomForest(x, y, seed = 123, samsungData.test.hex)
#   write(paste("test_model",model.test@model_id, sep="\t"), "model_id.txt", append = TRUE)
# })

# prediction time
ptm <- proc.time() # start timer
test.predicted = h2o.predict(model.with.3.features, newdata = samsungData.test.hex)
t.reduced = proc.time() - ptm # elapsed is the 'real' time
t.reduced
#summary(test.predicted)

# accuracy
test.accuracy = sum(test.predicted[,'predict']==samsungData.test.hex[,ncol])/dim(samsungData.test.hex[,ncol])[1]
test.accuracy
# confusion matrix
table(as.matrix(test.predicted[,'predict']),as.matrix(samsungData.test.hex[,ncol]))# need as.marix because h2o's result is environment and can't be put in table

```

#### compare with baseline
```{r baseline test}
ptm <- proc.time() # start timer
test.predicted.baseline = h2o.predict(model, newdata = samsungData.test.hex)
t.baseline = proc.time() - ptm # elapsed is the 'real' time
t.baseline
#summary(test.predicted.baseline)

# accuracy
test.accuracy.baseline = sum(test.predicted.baseline[,'predict']==samsungData.test.hex[,ncol])/dim(samsungData.test.hex[,ncol])[1]
test.accuracy.baseline
# confusion matrix
table(as.matrix(test.predicted.baseline[,'predict']),as.matrix(samsungData.test.hex[,ncol]))# need as.marix because h2o's result is environment and can't be put in table

```
- test accuracy: reduced model = `test.accuracy`
- test accuracy: full model = `test.accuracy.baseline`
- train accuracy: reduced model = `accuracy.randForest[3,]`
- train accuracy: full model = `accuracy.randForest[3,]`

### anova accuracy
```{r accuracy test vs train}
test.accuracy
test.accuracy.baseline
accuracy.randForest[3,]
baseline_accuracy
x = c(3, "all", 3,"all")
y = as.numeric(c(test.accuracy, test.accuracy.baseline,accuracy.randForest[3,]$mean,baseline_accuracy$mean))
sd = as.numeric(c(0,0,accuracy.randForest[3,]$sd,baseline_accuracy$sd))
group = as.factor(c("test", "test", "train", "train"))
test.accuracy.df = data.frame(x=x,y=y,group=group)

qplot(test.accuracy.df$x,test.accuracy.df$y, color=test.accuracy.df$group, shape=test.accuracy.df$group)+labs( x="number of features used", y="Accuracy")+geom_errorbar(aes(x=x, ymin=y-sd, ymax=y+sd), width=0.25)+ scale_colour_discrete(guide=F)+geom_hline(yintercept = h,lty = "dashed",show.legend = TRUE )+geom_text(aes("all",h,label = "threshold = 0.8", vjust = -1))+scale_shape_discrete(name = "dataset")

```

#### Reduction percentage
```{r percent reduced}

percent_reduction_reduced = c((test.accuracy -as.numeric(accuracy.randForest[3,]$mean) )*100/as.numeric(accuracy.randForest[3,]$mean),as.numeric(accuracy.randForest[3,]$sd))
names(percent_reduction_reduced ) = c("mean", "sd")
percent_reduction_full =  c((test.accuracy.baseline - as.numeric( baseline_accuracy$mean))*100/as.numeric( baseline_accuracy$mean), as.numeric( baseline_accuracy$sd))
names(percent_reduction_full ) = c("mean", "sd")

percent_diff_test = (test.accuracy - test.accuracy.baseline)*100/ test.accuracy.baseline
percent_diff_train = (as.numeric(accuracy.randForest[3,]$mean) - as.numeric(baseline_accuracy$mean))*100/as.numeric(baseline_accuracy$mean)
percent_reduction_reduced  
percent_reduction_full
percent_diff_train
percent_diff_test

```


#### view the test results

##### confusion matrix
```{r confusion-matrix}
# h2o.confusionMatrix(model.test)
# test.accuracy = 1 - h2o.confusionMatrix(model.test)['Totals', 'Error']
# test.accuracy
```

## Compare Runtime in trainning and cross-validate
- in milliseconds
- baseline model: `r model@model$run_time`
- model with 1-feature: `r model.with.1.features@model$run_time`
- model with 2-feature: `r model.with.2.features@model$run_time`
- model with 3-feature: `r model.with.3.features@model$run_time`
- model with 4-feature: `r model.with.4.features@model$run_time`
- model with 5-feature: `r model.with.5.features@model$run_time`


```{r runtime size 1 to 5 featured models}
results = data.frame()
for (i in 1:length(model_list)){
  m = model_list[[i]]
  for (this.m in h2o.cross_validation_models(m)){
    results = rbind.data.frame(results, c(this.m@model$run_time, i))
  }
}

colnames(results) = c("runtime", "group")
results$group = factor(results$group)
results
reduced_means = aggregate(x=results$runtime,by= list(results$group), FUN = mean)
reduced_sd = aggregate(x=results$runtime,by= list(results$group), FUN = sd)
reduced_results = cbind.data.frame(reduced_means$x, reduced_sd$x)
colnames(reduced_results) = c("mean", "sd")
```

```{r runtime plot compare 1}
x  = rownames(reduced_results)
y = reduced_results[,1]
sd = reduced_results[,2]
qplot(x,y)+geom_errorbar(aes(x=x, ymin=y-sd, ymax=y+sd), width=0.25)+labs(title = "Runtimes of models with different number of features (1-5)", x="number of features used", y="runtime(msecs)")


```

- runtime is not equal
```{r runtime reduced model anova}
selected_aov = aov(runtime ~ factor(group), data = results)
selected_aov
summary(selected_aov)
```

###### between reduced vs not reduced
```{r runtime reduced vs full anova}
full_vs_reduce = results
full_vs_reduce$group = as.character(full_vs_reduce$group )
for (m in h2o.cross_validation_models(model)){
  full_vs_reduce= rbind.data.frame(full_vs_reduce, c(m@model$run_time, "all"))
}
full_vs_reduce$runtime= as.numeric(full_vs_reduce$runtime )


all_means = aggregate(x=full_vs_reduce$runtime,by= list(full_vs_reduce$group), FUN = mean)
all_sd = aggregate(x=full_vs_reduce$runtime,by= list(full_vs_reduce$group), FUN = sd)
all_results = cbind.data.frame(all_means$x, all_sd$x)
colnames(all_results) = c("mean", "sd")
rownames(all_results) = all_means$Group.1
```

```{r runtime plot compare 2}
x  = rownames( all_results)
y =  all_results[,1]
sd =  all_results[,2]
qplot(x,y)+labs(title = "Runtimes of models with different numbers of features", x="number of features used", y="runtime (msecs)")+geom_errorbar(aes(x=x, ymin=y-sd, ymax=y+sd), width=0.25)
```

- runtime is not equal
```{r runtime full model anova}
## aov
full_aov = aov(runtime ~ factor(group), data = full_vs_reduce)
full_aov
summary(full_aov)
```


## Compare model sizes in trainning and cross-validate
- in bytes
- sizes are not significantly different
```{r size}
object.size(model)
temp = c()
for (m in h2o.cross_validation_models(model)){
  temp = c(temp, object.size(m))
}
print (c(mean(temp),sd(temp)))

```

```{r size 1 to 5 featured models}
results = data.frame()
for (i in 1:length(model_list)){
  m = model_list[[i]]
  for (this.m in h2o.cross_validation_models(m)){
    results = rbind.data.frame(results, c(object.size(this.m), i))
  }
}

colnames(results) = c("size", "group")
results$group = factor(results$group)
results
reduced_means = aggregate(x=results$size,by= list(results$group), FUN = mean)
reduced_sd = aggregate(x=results$size,by= list(results$group), FUN = sd)
reduced_results = cbind.data.frame(reduced_means$x, reduced_sd$x)
colnames(reduced_results) = c("mean", "sd")
```

```{r plot size compare 1}
x  = rownames(reduced_results)
y = reduced_results[,1]
sd = reduced_results[,2]
qplot(x,y)+geom_errorbar(aes(x=x, ymin=y-sd, ymax=y+sd), width=0.25)+labs(title = "Model size of models with different number of features (1-5)", x="number of features used", y="bytes")+scale_y_continuous(limits = c(min(results$size), max(results$size)))


```

```{r reduced model anova}
selected_aov = aov(size ~ factor(group), data = results)
selected_aov
summary(selected_aov)
```

###### between reduced vs not reduced
```{r reduced vs full anova}
full_vs_reduce = results
full_vs_reduce$group = as.character(full_vs_reduce$group )
for (m in h2o.cross_validation_models(model)){
  full_vs_reduce= rbind.data.frame(full_vs_reduce, c(object.size(m), "all"))
}
full_vs_reduce$size= as.numeric(full_vs_reduce$size )


all_means = aggregate(x=full_vs_reduce$size,by= list(full_vs_reduce$group), FUN = mean)
all_sd = aggregate(x=full_vs_reduce$size,by= list(full_vs_reduce$group), FUN = sd)
all_results = cbind.data.frame(all_means$x, all_sd$x)
colnames(all_results) = c("mean", "sd")
rownames(all_results) = all_means$Group.1
```

```{r plot size compare 2}
x  = rownames( all_results)
y =  all_results[,1]
sd =  all_results[,2]
qplot(x,y)+geom_errorbar(aes(x=x, ymin=y-sd, ymax=y+sd), width=0.25)+labs(title = "Model size of models with different numbers of features", x="number of features used", y="bytes")+scale_y_continuous(limits = c(min(full_vs_reduce$size), max(full_vs_reduce$size)))
```

```{r full model anova}
## aov
full_aov = aov(size ~ factor(group), data = full_vs_reduce)
full_aov
summary(full_aov)
```


##### Save models to local machine
```{r savemodel, eval=FALSE}
h2o.saveModel(model,paste(getwd(), deparse(substitute(model)),sep="_"))
for (m in model_list){
  h2o.saveModel(m,paste(getwd(),deparse(substitute(m)), sep="_"))
}
h2o.saveModel(model.test,paste(getwd(),"model.test", sep="_"))
for (m in model_list_var){
  h2o.saveModel(m,paste(getwd(),deparse(substitute(m)), sep="_"))
}
for (m in model_list_abs_cv){
  h2o.saveModel(m,paste(getwd(),deparse(substitute(m)), sep="_"))
}

```

------------------------------
## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

